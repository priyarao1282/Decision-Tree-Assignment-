{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRzxU2iP28kx"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "### **Decision Tree | Assignment**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **Question 1: What is a Decision Tree, and how does it work in the context of classification?**\n",
        "\n",
        "**Answer:**\n",
        "A **Decision Tree** is a supervised learning algorithm used for **classification and regression tasks**. It works by splitting the data into subsets based on the value of input features, creating a tree-like model of decisions.\n",
        "\n",
        "In **classification**, a Decision Tree divides the dataset into smaller groups by asking a series of questions (conditions) about the features. Each internal node represents a decision (test on an attribute), each branch represents the outcome of the test, and each leaf node represents a class label.\n",
        "\n",
        "**Working process:**\n",
        "\n",
        "1. The algorithm selects the best feature to split the data using impurity measures (like **Gini** or **Entropy**).\n",
        "2. It keeps splitting until all records belong to the same class or stopping conditions are met.\n",
        "3. The final model can be visualized as a tree, making it easy to interpret and explain.\n",
        "\n",
        "---\n",
        "\n",
        "### **Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?**\n",
        "\n",
        "**Answer:**\n",
        "**Gini Impurity:**\n",
        "It measures how often a randomly chosen element from the dataset would be incorrectly labeled if it were randomly labeled according to the distribution of labels.\n",
        "[\n",
        "Gini = 1 - \\sum p_i^2\n",
        "]\n",
        "Where ( p_i ) is the probability of each class.\n",
        "\n",
        "* Gini = 0 → perfect purity (only one class present).\n",
        "* Higher Gini = more mixed classes.\n",
        "\n",
        "**Entropy:**\n",
        "It measures the amount of uncertainty or randomness in the data.\n",
        "[\n",
        "Entropy = -\\sum p_i \\log_2(p_i)\n",
        "]\n",
        "\n",
        "* Entropy = 0 → perfectly pure node.\n",
        "* Higher entropy → higher disorder.\n",
        "\n",
        "**Impact on Splits:**\n",
        "Decision Trees use these measures to decide **where to split**. The feature and threshold that **reduce impurity the most** (highest **Information Gain**) are chosen for splitting.\n",
        "\n",
        "---\n",
        "\n",
        "### **Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "| Aspect           | Pre-Pruning                                                                                       | Post-Pruning                                                                          |\n",
        "| ---------------- | ------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------- |\n",
        "| **Definition**   | Stops the tree from growing once a certain condition is met (e.g., max_depth, min_samples_split). | Grows a full tree first and then removes branches that do not contribute to accuracy. |\n",
        "| **When Applied** | During training.                                                                                  | After training.                                                                       |\n",
        "| **Advantage**    | Saves computation time and prevents overfitting early.                                            | Improves generalization by simplifying the model.                                     |\n",
        "\n",
        "**Example Advantage:**\n",
        "\n",
        "* **Pre-Pruning:** Reduces overfitting during training by setting `max_depth`.\n",
        "* **Post-Pruning:** Results in a simpler, more interpretable model.\n",
        "\n",
        "---\n",
        "\n",
        "### **Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?**\n",
        "\n",
        "**Answer:**\n",
        "**Information Gain (IG)** measures how much “information” a feature gives us about the target variable after a split. It’s calculated as:\n",
        "[\n",
        "Information\\ Gain = Entropy(parent) - \\sum \\frac{n_i}{n} \\times Entropy(child_i)\n",
        "]\n",
        "It represents the **reduction in impurity** achieved by partitioning the data based on a specific feature.\n",
        "\n",
        "**Importance:**\n",
        "\n",
        "* The higher the IG, the better the feature separates the data.\n",
        "* Decision Trees use IG to select the best attribute at each split, ensuring the tree learns meaningful patterns.\n",
        "\n",
        "---\n",
        "\n",
        "### **Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?**\n",
        "\n",
        "**Answer:**\n",
        "**Applications:**\n",
        "\n",
        "* **Healthcare:** Disease prediction and diagnosis.\n",
        "* **Finance:** Credit risk scoring and fraud detection.\n",
        "* **Marketing:** Customer segmentation and churn analysis.\n",
        "* **Education:** Predicting student performance.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* Easy to interpret and visualize.\n",
        "* Handles both numerical and categorical data.\n",
        "* Requires little data preprocessing.\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "* Prone to overfitting if not pruned.\n",
        "* Sensitive to small data changes.\n",
        "* Can create complex trees for large datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### **Question 6: Write a Python program to:**\n",
        "\n",
        "* Load the Iris Dataset\n",
        "* Train a Decision Tree Classifier using the Gini criterion\n",
        "* Print the model’s accuracy and feature importances\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Feature Importances:\", clf.feature_importances_)\n",
        "```\n",
        "\n",
        "**Sample Output:**\n",
        "\n",
        "```\n",
        "Accuracy: 1.0\n",
        "Feature Importances: [0.02 0.00 0.43 0.55]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Question 7: Write a Python program to:**\n",
        "\n",
        "* Load the Iris Dataset\n",
        "* Train a Decision Tree Classifier with `max_depth=3`\n",
        "* Compare its accuracy to a fully-grown tree\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "# Full Tree\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "full_acc = accuracy_score(y_test, full_tree.predict(X_test))\n",
        "\n",
        "# Pruned Tree\n",
        "pruned_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "pruned_tree.fit(X_train, y_train)\n",
        "pruned_acc = accuracy_score(y_test, pruned_tree.predict(X_test))\n",
        "\n",
        "print(\"Full Tree Accuracy:\", full_acc)\n",
        "print(\"Pruned Tree (max_depth=3) Accuracy:\", pruned_acc)\n",
        "```\n",
        "\n",
        "**Sample Output:**\n",
        "\n",
        "```\n",
        "Full Tree Accuracy: 1.0\n",
        "Pruned Tree (max_depth=3) Accuracy: 0.9667\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Question 8: Write a Python program to:**\n",
        "\n",
        "* Load the Boston Housing Dataset\n",
        "* Train a Decision Tree Regressor\n",
        "* Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = load_boston()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"Feature Importances:\", regressor.feature_importances_)\n",
        "```\n",
        "\n",
        "**Sample Output:**\n",
        "\n",
        "```\n",
        "Mean Squared Error: 16.8\n",
        "Feature Importances: [0.03 0.00 0.00 0.00 0.67 0.00 0.13 0.00 0.02 0.00 0.00 0.10 0.05]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Question 9: Write a Python program to:**\n",
        "\n",
        "* Load the Iris Dataset\n",
        "* Tune the Decision Tree’s `max_depth` and `min_samples_split` using GridSearchCV\n",
        "* Print the best parameters and model accuracy\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5],\n",
        "    'min_samples_split': [2, 3, 4]\n",
        "}\n",
        "\n",
        "# Grid search\n",
        "grid = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=3)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "# Evaluate model\n",
        "best_model = grid.best_estimator_\n",
        "print(\"Best Model Accuracy:\", accuracy_score(y_test, best_model.predict(X_test)))\n",
        "```\n",
        "\n",
        "**Sample Output:**\n",
        "\n",
        "```\n",
        "Best Parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
        "Best Model Accuracy: 0.9667\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease...**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Step 1: Handle Missing Values**\n",
        "\n",
        "* Use **mean/median imputation** for numerical columns.\n",
        "* Use **mode imputation** or a special category (“Unknown”) for categorical variables.\n",
        "* Alternatively, use `SimpleImputer` from `sklearn.impute`.\n",
        "\n",
        "**Step 2: Encode Categorical Features**\n",
        "\n",
        "* Convert text-based features into numeric form using **Label Encoding** or **One-Hot Encoding** (`pd.get_dummies()` or `OneHotEncoder`).\n",
        "\n",
        "**Step 3: Train a Decision Tree Model**\n",
        "\n",
        "* Split data into training and testing sets.\n",
        "* Train a `DecisionTreeClassifier` using appropriate parameters.\n",
        "\n",
        "**Step 4: Tune Hyperparameters**\n",
        "\n",
        "* Use `GridSearchCV` to tune `max_depth`, `min_samples_split`, and `criterion`.\n",
        "\n",
        "**Step 5: Evaluate Performance**\n",
        "\n",
        "* Use metrics like **accuracy**, **precision**, **recall**, and **F1-score**.\n",
        "* Use **confusion matrix** to check classification results.\n",
        "\n",
        "**Business Value:**\n",
        "\n",
        "* The model helps doctors identify high-risk patients quickly.\n",
        "* Improves early diagnosis, saves time and cost.\n",
        "* Helps in resource allocation and preventive healthcare planning.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    }
  ]
}